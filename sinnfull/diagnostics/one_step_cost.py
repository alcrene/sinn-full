# -*- coding: utf-8 -*-
# ---
# jupyter:
#   jupytext:
#     formats: py:percent
#     notebook_metadata_filter: -jupytext.text_representation.jupytext_version
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
#   kernelspec:
#     display_name: Python (sinnfull)
#     language: python
#     name: sinnfull
# ---

# %% [markdown]
# # One-step-forward likelihood test
#
# This module implements a simple but effective consistency check between a model's update equations and the objective function used to fit that model. For the purpose of this test, variable dimensions are concatenated, such that if the model involves 2 variables with respectively 2 and 3 dimensions, we treat it as predicting an $M=5$ dimensional distribution. For some given $t$, the goal is to estimates the $M$ marginals of $p(X_{t+1}^m | X_t^m)$ in two ways:
#
# 1. By integrating the model up to $t$, and then running the simulator for time $t+1$ $N$ times. Binning the results (using NumPy's 'auto' algorithm to determine the bin edges $\{\bar{x}^m\}$) gives estimates
#
#   :::{math}
#   :label: eq:one-step-forward-pmc
#   \begin{gathered}\hat{p}_{\text{MC}}(\bar{x}^m_l \leq X_{t+1}^m < \bar{x}^m_{l+1} | X_t=\{x_t\}) \,.\end{gathered}
#   :::
#   
# 2. By evaluating the objective function at each of the points generated by the simulator, giving $p_{\text{obj}}(X_{t+1}=x_{t+1} | X_t=\{x_t\})$. Taking the same bin edges, we average within each bin to get
#   
#   :::{math}
#   :label: eq:one-step-forward-pobj
#   \begin{gathered}\hat{p}_{\text{obj}}(\bar{x}^m_l \leq X_{t+1}^m < \bar{x}^m_{l+1} | X_t=\{x_t\}) = \frac{1}{|B^m_l|} \sum_{x_i \in B^m_l} p_{\text{obj}}(X_{t+1}=x_{t+1} | X_t=\{x_t\})\,,\end{gathered}
#   :::
#   
#   where $B^m_l = \{x_i | \bar{x}^m_i \leq x^m_i < \bar{x}^m_{i+1}\}$ is the $l$-th bin for dimension $m$.
#
# For [$p_{MC}$](eq:one-step-forward-pmc), we can in fact do better, and feed the samples to a kernel density estimator (a histogram is also computed, but only to obtain bin edges for $p_{\text{obj}}$). This produces a smooth distribution, which is easier to visually compare to the histograms of [$p_{obj}$](eq:one-step-forward-pobj) when the two distributions are plotted over each other. If we see mismatch between the distributions, fits are unlikely to produce meaningful results.
#
# :::{Note}
# If the objective function is the model's actual likelihood, the two distributions should match exactly. Otherwise, judgement is needed to determine what constitutes “matching distributions”. For example, with an ad-hoc squared-error loss, the distribution tails are likely to decay at different rates, but their modes should be aligned.  
# :::

# %%
if __name__ == "__main__":
    import sinnfull
    # This test is around 2.5 faster with numpy vs theano
    sinnfull.setup('numpy', view_only=True)

# %%
from typing import Union, Callable
from warnings import warn
import functools
import numpy as np
from scipy import integrate
import pandas as pd
from tqdm.auto import tqdm

import theano_shim as shim
from mackelab_toolbox import iotools
from sinn.histories import TimeAxis
from sinnfull.models import models, Model, ModelSpec, ObjectiveFunction, get_objectives
import sinnfull.rng
from sinnfull.utils import add_to, add_property_to
from sinnfull.tasks import CreateModel
from sinnfull.viz import pretty_names
from sinnfull.viz.utils import get_histdims

# %%
import holoviews as hv
if __name__ == "__main__":
    from IPython.display import display
    hv.extension('bokeh')


# %%
class OneStepForwardLogp:
    def __init__(self,
                 model: Union[Model, ModelSpec],
                 logp: Union[ObjectiveFunction, 'ObjectiveSelector', list],
                 T = 1.,
                 step = 0.01
                 ):
        """
        If `logp` is specified as a list, the sum of its elements is used.
        """
        if not isinstance(model, Model):
            time = TimeAxis(min=0., max=T, step=step)
            model = CreateModel(time=time, model_selector=model, rng_key=((0,))
                               ).run()
        elif T != 1. or step != 0.01:  # Keep in sync with defaults
            warn("Arguments `T` and `step` are ignored when a model instance "
                 "is provided.")
        self.model = model
        
        # if not isinstance(logp, ObjectiveFunction):
        # if not isinstance(logp, Callable):
        #     if isinstance(logp, Callable):
        #         raise TypeError("`logp` argument must be an ObjectiveFunction. "
        #                         f"Received '{type(logp)}'")
        #     logp = objectives[logp]
        if not isinstance(logp, Callable):
            logp = sum(get_objectives(logp))
            
            # If using Theano, compile the objective functions. This massively speeds up the test.
            if shim.config.library == 'theano':
                cgraph = logp(model, model.num_tidx)
                cgraph = shim.graph.clone(cgraph, {model.num_tidx: model.curtidx_var})
                f = shim.graph.compile([model.curtidx_var], cgraph)
                @functools.wraps(logp)
                def compiled_logp(model, k, f=f, model_compiled_against=model):
                    assert model is model_compiled_against
                    return f(k)
                logp = compiled_logp
            
        self.logp = logp

        self.set_random_init_condition()
        
        self._samples = []
        self._logps = []


# %% [markdown]
# Set a random initial value

    # %%
    @add_to('OneStepForwardLogp')
    def set_random_init_condition(self):
        for h in self.model.history_set:
            if h.t0idx > 0:
                h[:0] = rng.normal(size=(h.t0idx,)+h.shape)

# %% [markdown]
# Place the current time at $t$.

    # %%
    @add_to('OneStepForwardLogp')
    def set_t(self, t):
        if not hasattr(t, 'magnitude'):  # If 't' is not a unit
            t *= self.model.time.unit    # Works also if time.unit is unitless
        tidx = self.model.get_tidx(t, allow_rounding=True)
        self.model.clear(after=tidx)
        self.model.integrate(upto=tidx)

# %% [markdown]
# Histories are flattened into one big multivariate distribution

    # %%
    @add_property_to('OneStepForwardLogp')
    def kdims(self):
        """Flattened history dimensions"""
        return get_histdims(self.model.unlocked_histories)
        # return [
        #     hv.Dimension(f"{pretty_names.unicode.get(h.name)}{'_' if len(idx) else ''}{''.join(str(i) for i in idx)}",
        #                  label=pretty_names.wrap(
        #                      pretty_names.get(h.name)+pretty_names.index_str(idx, prefix='_')))
        #     for h in self.model.unlocked_histories for idx in index_iter(h.shape)]

# %% [markdown]
# Sample the next step of the simulation multiple times.

    # %%
    @add_to('OneStepForwardLogp')
    def sample_next_step(self, N, keep_old_samples: bool=False):
        if keep_old_samples:
            if len(self._samples) != len(self._logps):
                raise RuntimeError("`_samples` and `_parr` attribute have desynchronized.")
        else:
            self._samples = []
            self._logps = []

        tidx = self.model.cur_tidx

        for _ in tqdm(range(N)):
            self.model.integrate(tidx+1, histories='all')
            self._samples.append([v for h in self.model.unlocked_histories
                                    for v in h[tidx+1].eval().flat])  # For loop order must match `self.kdims`
            self._logps.append(self.logp(self.model, tidx+1))
            self.model.clear(after=tidx)
            
        self.samples = pd.DataFrame(self._samples, columns=[d.name for d in self.kdims])
        self.parr = np.exp(self._logps)

# %% [markdown]
# Construct the histogram of $X_{t+1}$ points based on the simulation. We use the bin edges of the `Histogram` to bin the $\log p$ values below. The `Distribution` object uses a kernel density estimator to produce a smoother, higher resolution estimate of $p(X_{t+1} | X_t)$; it looks nicer in figures, and is easier to compare to the estimate obtained from the likelihood. One can check that the two represent the same distribution by plotting them overlaid:
# ```python
# (mc_marginal_hists.opts(color='orange') * mc_marginal_dists) \
#     .layout().cols(2)
# ```

    # %%
    @add_property_to('OneStepForwardLogp')
    def mc_marginal_hists(self):
        return hv.HoloMap(
            {d.name: hv.Histogram(np.histogram(self.samples[d.name].values, bins='auto', density=True),
                                  kdims=[d], vdims=['p'], label="likelihood (MC)")
             for d in self.kdims},
            kdims=['dimension']
            ).opts(framewise=True)

    # %%
    @add_property_to('OneStepForwardLogp')
    def mc_marginal_dists(self):
        return hv.HoloMap(
            {d.name: hv.Distribution(self.samples[d.name].values,
                                     kdims=[d], vdims=['p'], label="likelihood (MC)")
             for d in self.kdims},
            kdims=['dimension']
            ).opts(framewise=True)

# %% [markdown]
# Marginalize the likelihood by evaluating it at the sampled points.
#
# For each dimension, we obtain the bins edges computed above and bin the likelihood values according to the samples' values along that dimension (so, if there are $M$ dimensions, `parr` will be binned $M$ different ways).

# %% [markdown]
# Samples are binned according to ≤ right edge, so index 1 corresponds to the left-most bin.

# %% [markdown]
# In theory, marginalizing the likelihood should still yield a proper probability distribution. In practice this is not the case for two reasons:
# - The objective function `logp` may be unnormalized;
# - The number of samples is finite; especially the tails will be poorly estimated, since number of samples ~ $p$.
#
# For these reasons we normalize by the numerical integral.

    # %%
    @add_property_to('OneStepForwardLogp')
    def p_marginals(self):
        p_marginals = {}

        for dim in self.kdims:
            edges = self.mc_marginal_hists[dim.name].edges
            centers = (edges[:-1]+edges[1:])/2
            
            if len(centers) == 1:
                warn(
                    f"All {dim} simulations samples have the same value: {centers[0]}. "
                    "Most likely, either the simulator is not actually "
                    "stochastic, or an insufficient number of samples were used.")
            elif len(centers) < 20:
                warn(
                    f"{dim} simulations samples were aggregated into only {len(centers)} "
                    "bins. Most likely more samples are needed.")

            binned_idcs = np.searchsorted(edges, self.samples[dim.name])

            binned_parrs = (self.parr[binned_idcs==i]
                            for i in range(1, len(edges)))
            p_marginal_μ = [parr.mean() if len(parr) else 0
                            for parr in binned_parrs]
            # Note: Although it's tempting to compute std on the binned samples to get
            #       error bars, that value is not meaningful.
            #       One could consider however a bootstrap estimate

            I = integrate.trapz(p_marginal_μ, centers)
            p_marginal_μ /= I

            density_data = np.stack((centers, p_marginal_μ), axis=1)

            p_marginals[dim.name] = hv.Curve(
                density_data, kdims=[dim], vdims=['p'], label="objective function")

        return hv.HoloMap(p_marginals, kdims=['dimension'])

# %% [markdown]
# Overlay the estimates of $P(X_{t+1} | X_t)$ based on the (Monte Carlo) likelihood and the objective function, to see if they match. Note that if we are using a true likelihood, these really should match exactly; errors on secondary terms in either the simulation or objective function equations may cause only slight variations.
#
# Beyond implementation correctness, the shape of this distribution can also help identify issues when fitting latents. Distributions which are strongly peaked along one component will tend to favour fitting that component to the detriment of others. Moreover, if the support of $P(X_{t+1} | X_t)$ is too small along a given component (which may be related to it being too peaked), then when simulations are far from observations, there may be little signal with which to compute gradients.
#
# Limitations:
# - These distributions are drawn for one particular time point, for one particular set of parameters. It is worth running this test multiple times with different parameters. Whether it is also worth running the test for multiple time points depends on whether the system is ergodic.
# - “Peakiness” is only indicative for the latent fit. For a similar assessment of likelihood over parameters, one would have to plot distributions over the parameters.

# %%
if __name__ == "__main__":
    test = OneStepForwardLogp(
        model = ["GaussianWhiteNoise"],
        logp = ['GWN', 'log L'],
        T = 1., step = 0.01)

    test.set_t(0.5)
    test.sample_next_step(1000)

# %%
if __name__ == "__main__":
    fig = (test.mc_marginal_dists * test.p_marginals.opts(color='orange', line_width=3)) \
          .layout().cols(2).opts(axiswise=True) \
          .opts(hv.opts.Curve(axiswise=True), hv.opts.Distribution(axiswise=True),
                hv.opts.Overlay(axiswise=True))
    display(fig)

# %%
