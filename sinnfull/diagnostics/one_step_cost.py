# -*- coding: utf-8 -*-
# ---
# jupyter:
#   jupytext:
#     formats: ipynb,py:percent
#     notebook_metadata_filter: -jupytext.text_representation.jupytext_version
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
#       jupytext_version: 1.6.0
#   kernelspec:
#     display_name: Python (sinn-full)
#     language: python
#     name: sinn-full
# ---

# %% [markdown]
# # One-step-forward likelihood test
#
# This notebook performs a simple but effective consistency check between a model's update equations and the likelihood used to fit that model. For the purpose of this test, variable dimensions are concatenated, such that if the model involves 2 variables with respectively 2 and 3 dimensions, we treat it as predicting an $M=5$ dimensional distribution. For some given $t$, the goal is to estimates the $M$ marginals of $p(X_{t+1}^m | X_t^m)$ in two ways:
#
# 1. By integrating the model up to $t$, and then running the simulator for time $t+1$ $N$ times. Binning the results (using NumPy's 'auto' algorithm to determine the bin edges $\{\bar{x}^m\}$) gives estimates
#   $$\hat{p}_{\text{sim}}(\bar{x}^m_l \leq X_{t+1}^m < \bar{x}^m_{l+1} | X_t=\{x_t\}) \,.$$
# 2. By evaluating the objective function at each of the points generated by the simulator, giving $p_{\text{obj}}(X_{t+1}=x_{t+1} | X_t=\{x_t\})$. Taking the same bin edges, we average within each bin to get
#   $$\hat{p}_{\text{obj}}(\bar{x}^m_l \leq X_{t+1}^m < \bar{x}^m_{l+1} | X_t=\{x_t\}) = \frac{1}{|B^m_l|} \sum_{x_i \in B^m_l} p_{\text{obj}}(X_{t+1}=x_{t+1} | X_t=\{x_t\})\,,$$
#   where $B^m_l = \{x_i | \bar{x}^m_i \leq x^m_i < \bar{x}^m_{i+1}\}$ is the $l$-th bin for dimension $m$.
#
# For (1), we can in fact do better, and use a kernel density estimator instead of a histogram. This produces a smooth distribution, which is easier to visually compare to the histograms of (2) when the two distributions are plotted over each other. If we see any mismatch between the distributions, fits are unlikely to produce meaningful results.

# %%
if __name__ == "__main__":
    import sinnfull
    # This test is around 2.5 faster with numpy vs theano
    sinnfull.setup('numpy', view_only=True)

# %%
import numpy as np
from scipy import integrate
import pandas as pd
from tqdm.auto import tqdm

from mackelab_toolbox.utils import index_iter
from mackelab_toolbox import iotools
from sinn.histories import TimeAxis
import sinnfull.models
import sinnfull.rng
from sinnfull.viz import pretty_names
from sinnfull.utils import add_to, add_property_to

# %%
import holoviews as hv
if __name__ == "__main__":
    from IPython.display import display
    hv.extension('bokeh')


# %%
class OneStepForwardLogp:
    def __init__(self, model_name,
                 logp_name = 'logp_forward_se',
                 T = 1.,
                 step = 0.01
                 ):
        ModelClass = iotools._load_types[model_name]
        self.logp = sinnfull.models.objectives[model_name][logp_name]

        self.model = self.instantiate_model(ModelClass, T, step)
        self.initialize_model()
        
        self._samples = []
        self._logps = []


# %% [markdown]
# Instantiate a test model

    # %%
    @add_to('OneStepForwardLogp')
    @staticmethod
    def instantiate_model(ModelClass, T, step):
        time = TimeAxis(min=0., max=1., step=step)
        rng = sinnfull.rng.get_sim_rng((0,), exists_ok=True)
        return ModelClass(time=time,
                          params=ModelClass.get_test_parameters(),
                          rng=rng)

# %% [markdown]
# Set a random initial value

    # %%
    @add_to('OneStepForwardLogp')
    def initialize_model(self):
        for h in self.model.history_set:
            if h.t0idx > 0:
                h[:0] = rng.normal(size=(h.t0idx,)+h.shape)

# %% [markdown]
# Place the current time at $t$.

    # %%
    @add_to('OneStepForwardLogp')
    def set_t(self, t):
        tidx = self.model.get_tidx(t)
        self.model.clear(after=tidx)
        self.model.integrate(upto=tidx)

# %% [markdown]
# Histories are flattened into one big multivariate distribution

    # %%
    @add_property_to('OneStepForwardLogp')
    def kdims(self):
        """Flattened history dimensions"""
        return [
            hv.Dimension(f"{pretty_names.unicode.get(h.name)}{'_' if len(idx) else ''}{''.join(str(i) for i in idx)}",
                         label=pretty_names.wrap(
                             pretty_names.get(h.name)+pretty_names.index_str(idx, prefix='_')))
            for h in self.model.history_set for idx in index_iter(h.shape)]

# %% [markdown]
# Sample the next step of the simulation multiple times.

    # %%
    @add_to('OneStepForwardLogp')
    def sample_next_step(self, N, keep_old_samples: bool=False):
        if keep_old_samples:
            if len(self._samples) != len(self._parr):
                raise RuntimeError("`_samples` and `_parr` attribute have desynchronized.")
        else:
            self._samples = []
            self._logps = []

        tidx = self.model.cur_tidx

        for _ in tqdm(range(N)):
            self.model.integrate(tidx+1, histories='all')
            self._samples.append([v for h in self.model.history_set
                                    for v in h[tidx+1].eval().flat])  # For loop order must match `self.kdims`
            self._logps.append(self.logp(self.model, tidx+1))
            self.model.clear(after=tidx)
            
        self.samples = pd.DataFrame(self._samples, columns=[d.name for d in self.kdims])
        self.parr = np.exp(self._logps)

# %% [markdown]
# Construct the histogram of $X_{t+1}$ points based on the simulation. We use the bin edges of the `Histogram` to bin the $\log p$ values below. The `Distribution` object uses a kernel density estimator to produce a smoother, higher resolution estimate of $p(X_{t+1} | X_t)$; it looks nicer in figures, and is easier to compare to the estimate obtained from the likelihood. One can check that the two represent the same distribution by plotting them overlaid:
# ```python
# (sim_marginal_hists.opts(color='orange') * sim_marginal_dists) \
#     .layout().cols(2)
# ```

    # %%
    @add_property_to('OneStepForwardLogp')
    def sim_marginal_hists(self):
        return hv.HoloMap(
            {d.name: hv.Histogram(np.histogram(self.samples[d.name].values, bins='auto', density=True),
                                  kdims=[d], vdims=['p'], label="simulation")
             for d in self.kdims},
            kdims=['dimension']
            ).opts(framewise=True)

    # %%
    @add_property_to('OneStepForwardLogp')
    def sim_marginal_dists(self):
        return hv.HoloMap(
            {d.name: hv.Distribution(self.samples[d.name].values,
                                     kdims=[d], vdims=['p'], label="simulation")
             for d in self.kdims},
            kdims=['dimension']
            ).opts(framewise=True)

# %% [markdown]
# Marginalize the likelihood by evaluating it at the sampled points.
#
# For each dimension, we obtain the bins edges computed above and bin the likelihood values according to the samples' values along that dimension (so, if there are $M$ dimensions, `parr` will be binned $M$ different ways).

# %% [markdown]
# Samples are binned according to ≤ right edge, so index 1 corresponds to the left-most bin.

# %% [markdown]
# In theory, marginalizing the likelihood should still yield a proper probability distribution. In practice this is not the case for two reasons:
# - The objective function `logp` may be unnormalized;
# - The number of samples is finite; especially the tails will be poorly estimated, since number of samples ~ $p$.
#
# For these reasons we normalize by the numerical integral.

    # %%
    @add_property_to('OneStepForwardLogp')
    def p_marginals(self):
        p_marginals = {}

        for dim in self.kdims:
            edges = self.sim_marginal_hists[dim.name].edges
            centers = (edges[:-1]+edges[1:])/2

            binned_idcs = np.searchsorted(edges, self.samples[dim.name])

            p_marginal_μ = np.nan_to_num([self.parr[binned_idcs==i].mean() for i in range(1, len(edges))])
            # Note: Although it's tempting to compute std on the binned samples to get
            #       error bars, that value is not meaningful.
            #       One could consider however a bootstrap estimate

            I = integrate.trapz(p_marginal_μ, centers)
            p_marginal_μ /= I

            density_data = np.stack((centers, p_marginal_μ), axis=1)

            p_marginals[dim.name] = hv.Curve(
                density_data, kdims=[dim], vdims=['p'], label="likelihood")

        return hv.HoloMap(p_marginals, kdims=['dimension'])

# %% [markdown]
# Overlay the estimates of $P(X_{t+1} | X_t)$ based on the simulation and the likelihood (objective function), to see if they match. Note that if we are using a true likelihood, these really should match exactly; errors on secondary terms in either the simulation or likelihood equations may cause only slight variations.
#
# Beyond implementation correctness, the shape of this distribution can also help identify issues when fitting latents. Distributions which are strongly peaked along one component will tend to favour fitting that component to the detriment of others. Moreover, if the support of $P(X_{t+1} | X_t)$ is too small along a given component (which may be related to it being too peaked), then when simulations are far from observations, there may be little signal with which to compute gradients.
#
# Limitations:
# - These distributions are drawn for one particular time point, for one particular set of parameters. It is worth running this test multiple times with different parameters. Whether it is also worth running the test for multiple time points depends on whether the system is ergodic.
# - “Peakiness” is only indicative for the latent fit. For a similar assessment of likelihood over parameters, one would have to plot distributions over the parameters.

# %%
if __name__ == "__main__":
    test = OneStepForwardLogp(
        model_name = 'OUInput', logp_name = 'logp_forward_se',
        T = 1., step = 0.01)

    test.set_t(0.5)
    test.sample_next_step(1000)

# %%
if __name__ == "__main__":
    fig = (test.sim_marginal_dists * test.p_marginals.opts(color='orange', line_width=3)) \
          .layout().cols(2).opts(axiswise=True) \
          .opts(hv.opts.Curve(axiswise=True), hv.opts.Area(axiswise=True))
    display(fig)

# %%
