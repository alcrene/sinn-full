# -*- coding: utf-8 -*-
# ---
# jupyter:
#   jupytext:
#     formats: py:percent,ipynb
#     notebook_metadata_filter: -jupytext.text_representation.jupytext_version
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
#   kernelspec:
#     display_name: Python (sinnfull)
#     language: python
#     name: sinnfull
# ---

# %% [markdown]
# # One-step-forward likelihood test

# %% [markdown] tags=["remove-cell"]
# > **Note**
# > This notebook is “report-ready”.  
# > Documentation cells for users (like this one) are already tagged for exclusion by Jupyter Book, so that generated reports contain only the relevant results.

# %% [markdown]
# Below we test the Wilson-Cowan model with Gaussian white noise input by comparing two distributions at a given time point $t$:
#
# - $p_{MC}(X(t))$: The distribution at time $t$ given a fixed history $\{X(s)\}_{s<t}$. This is a Monte Carlo estimate, obtained by repeatedly simulating the model's update for time $t$.
# - $p_{obj}(X(t))$: The objective function evaluated at time $t$, marginalized over each variable.
#
# :::{caution}  
# This test only verifies the consistency of the update and objective functions, as defined in the [*models*](../models) subpackage. No code from the optimizer is used; this includes the code involved in compiling the definitions in *models* into differentiable functions suitable for gradient-based optimization.
#
# Thus, passing this test is a necessary *but not sufficient* condition for the optimizer to evaluate the loss correctly. That said, since models are changed much more frequently than optimizers, it should catch most errors.  
# :::
#
# ::::{dropdown} Summary of the “one-step-ahead” test  
# (Copied from the test's [the implementation module](../diagnostics/one_step_cost.ipynb).)
#
# This test performs a consistency check between a model's update equations and the likelihood used to fit that model. For the purpose of this test, variable dimensions are concatenated, such that if the model involves 2 variables with respectively 2 and 3 dimensions, we treat it as predicting an $M=5$ dimensional distribution. For some given $t$, the goal is to estimates the $M$ marginals of $p(X_{t+1}^m | X_t^m)$ in two ways:
#
# 1. By integrating the model up to $t$, and then running the simulator for time $t+1$ $N$ times. Binning the results (using NumPy's 'auto' algorithm to determine the bin edges $\{\bar{x}^m\}$) gives estimates
#
#   :::{math}
#   :label: eq:one-step-forward-pmc-copy
#   \begin{gathered}\hat{p}_{\text{MC}}(\bar{x}^m_l \leq X_{t+1}^m < \bar{x}^m_{l+1} | X_t=\{x_t\}) \,.\end{gathered}
#   :::
#   
# 2. By evaluating the objective function at each of the points generated by the simulator, giving $p_{\text{obj}}(X_{t+1}=x_{t+1} | X_t=\{x_t\})$. Taking the same bin edges, we average within each bin to get
#   
#   :::{math}
#   :label: eq:one-step-forward-pobj-copy
#   \begin{gathered}\hat{p}_{\text{obj}}(\bar{x}^m_l \leq X_{t+1}^m < \bar{x}^m_{l+1} | X_t=\{x_t\}) = \frac{1}{|B^m_l|} \sum_{x_i \in B^m_l} p_{\text{obj}}(X_{t+1}=x_{t+1} | X_t=\{x_t\})\,,\end{gathered}
#   :::
#   
#   where $B^m_l = \{x_i | \bar{x}^m_i \leq x^m_i < \bar{x}^m_{i+1}\}$ is the $l$-th bin for dimension $m$.
#
# For [$p_{MC}$](eq:one-step-forward-pmc-copy), actually use a kernel density estimator instead of a histogram. This produces a smooth distribution, which is easier to visually compare to the histograms of [$p_{obj}$](eq:one-step-forward-pobj-copy) when the two distributions are plotted over each other. If we see mismatch between the distributions, fits are unlikely to produce meaningful results.
#
# :::{Note}
# If the objective function is the model's actual likelihood, the two distributions should match exactly. Otherwise, judgement is needed to determine what constitutes “matching distributions”. For example, with an ad-hoc squared-error loss, the distribution tails are likely to decay at different rates, but their modes should be aligned.  
# :::
#
# Beyond implementation correctness, the shape of this distribution can also help identify issues when fitting latents. For example, if the support of $P(X_{t+1} | X_t)$ is too small along a given component, then when simulations are far from observations, there may be little signal with which to compute gradients.  
# ::::

# %% [markdown]
# **Possible reasons for failing this test**
#
# - The update function is inconsistent with the objective. Typically this is due to a bug in the implementation of one of the two functions.
#   + This will be seen if the marginals of the objective function and the likelihood don't align.
# - An ad-hoc objective is too tight.  
#   When a true likelihood is not available, we may attempt a fit with an ad-hoc objective like a squared-error loss. This defines a kind of pseudo-likelihood (by taking the $\exp$ of the loss and normalizing). Fits can typically only converge to solutions where the stationary distributions has the same variance as the data, or this pseudo-likelihood – whichever is tighter. Hence it is important for ad-hoc objectives not to be tighter than the actual likelihood.  
#   This is similar to how too tight priors can prevent any a fit from finding the ground truth values, while too broad prior simply require more data to do so.
#   + Marginals of the objectives should be at least as broad as the empirical likelihood.
#   + In this situation, the mode of the likelihood may appear to align with the ground truth, but still conserve a small mis-alignment. This can be detected by sampling enough points that some will find a higher value of the objective.
#   + A more efficient test in this case may be the [latent gradient test](Check%20uniformity%20of%20gradients%20--%20example.ipynb).

# %% [markdown] tags=["remove-cell"]
# > **Note**  
# > This test is intended for general testing of a model implementation. If you wish to apply it to investigate a particular situation which occurred during a recorded run, see the [“optimizer objective” variant](./One-step-forward%20cost%20--run%20data).  

# %% tags=["remove-cell"]
import sinnfull
sinnfull.setup('theano', view_only=True)
    # Theano is faster, especially for large numbers of samples,
    # but Numpy is typically easier to debug.
    # Differences between the two should be considered bugs.

# %% tags=["remove-cell"]
import itertools
import functools
import numpy as np
import pandas as pd
import theano_shim as shim
from mackelab_toolbox.utils import index_iter, GitSHA
from sinnfull.models import models, TimeAxis, objectives, paramsets, get_objectives
from sinnfull.diagnostics.one_step_cost import OneStepForwardLogp
from sinnfull.viz import pretty_names
from sinnfull.viz.utils import get_histdim_name

# %% tags=["remove-input"]
import holoviews as hv
hv.extension('bokeh')
print("")  # Force a newline after the Holoviews logos in the html file

# %% [markdown]
# Select model parameters:

# %%
Θgwn = paramsets.GWN.rich
Θwc = paramsets.WC.rich

# %% [markdown] tags=["remove-cell"]
# Alternatively, we could also use random test parameters:
# ```python
# Θgwn = models.GWN.get_test_parameters()
# Θwc = models.WC.get_test_parameters()
# ```

# %% [markdown]
# Construct the model we want to test:

# %%
timeaxis = TimeAxis(min=0, max=1, step=2**-7)
gwn = models.GWN(time=timeaxis, params=Θgwn)
## To always use the same RNG seed (useful for debugging), use the lines below
#from sinnfull.rng import get_shim_rng
#gwn = models.GWN(time=timeaxis, params=Θgwn, rng=get_shim_rng((4,)))
wc = models.WC(time=timeaxis, params=Θwc, I=gwn.ξ)

# %%
model = models.ObservedDynamics(time=timeaxis,
                                params={'input':gwn.params, 'dynamics':wc.params},
                                input=gwn, dynamics=wc)

# %% [markdown]
# Select a time point. This should be between the min and max of the time axis.
# Also select the number of samples to draw for the test.

# %%
test_t = 0.5
num_test_samples = 4000

# %% [markdown] tags=["remove-cell"]
# Integrate the model. This will serve as the ground truth data.

# %% tags=["remove-cell"]
model.reseed_rngs(123)   # FIXME: This first integration isn't reproducible
model.integrate('end')
# Lock observed variables
wc.u.lock()
# Reset model Theano updates (in particular, reset the RNG to a different seed)
model.theano_reset()
model.reseed_rngs(246)

# %% [markdown]
# Run the test

# %% tags=["remove-cell"]
obj_list = get_objectives({'dynamics': {'WilsonCowan', 'se'},
                           'input': {'GaussianWhiteNoise'}})
# Note: in simple cases, it is also possible to retrieve objectives directly,
# e.g. `objectives.WC`

# %% tags=["remove-cell"]
# List format:
# - One entry for each model-objective pair we want to test
# - Each entry is composed of (model, objective, test label)
obj_tests = [#(model, obj_list[0], "wc_se"),
             #(model, obj_list[1], "gwn_logp"),
             (model, sum(obj_list), "wc_se+gwn_logp")
            ]

# %%
# If using Theano, compile the objective functions. This massively speeds up the test.
if shim.config.library == 'theano':
    new_tests = []
    for model, obj, obj_name in obj_tests:
        cgraph = obj(model, model.num_tidx)
        cgraph = shim.graph.clone(cgraph, {model.num_tidx: model.curtidx_var})
        f = shim.graph.compile([model.curtidx_var], cgraph)
        @functools.wraps(obj)
        def compiled_objective(model, k, f=f, model_compiled_against=model):
            assert model is model_compiled_against
            return f(k)
        new_tests.append((model, compiled_objective, obj_name))
    obj_tests = new_tests

# %% [markdown] tags=["remove-cell"]
# Store a ground truth value for each model used in a test, before the histories are cleared.

# %% tags=["remove-cell"]
_models = {id(m): m for m, _, _ in obj_tests}  # Dictionary serves to remove duplicates
ground_truth = {
    _model.name: {
        get_histdim_name(h, idx)
        : h.data[h.get_tidx(test_t, allow_rounding=True)+1][idx]
        for h in _model.unlocked_histories for idx in index_iter(h.shape)}
    for _model in _models.values()}

# %% [markdown]
# Store the value of the objective with the ground truth value, before the histories are cleared.

# %%
# FIXME: Assumes only one model
k = model.get_tidx(test_t, allow_rounding=True)
vals = ground_truth['ObservedDynamics']
ξk = np.array([vals['ξ_0'], vals['ξ_1']])
old_ξk = model.input.ξ.data[k+1]
model.input.ξ[k+1] = ξk

logp_gt = {}
for _model, obj, obj_name in obj_tests:
    k = model.get_tidx(test_t, allow_rounding=True)
    logp_gt[(_model.name, obj_name)] = obj(model, k+1)
    
model.input.ξ[k+1] = old_ξk

# %% tags=["remove-cell"]
frames = {}
higher_than_gt_frames = {}
for _model, obj, obj_name in obj_tests:
    test = OneStepForwardLogp(
        model = _model,
        logp = obj)
    test.set_t(test_t)
    test.sample_next_step(num_test_samples)
    gt = ground_truth[_model.name]
    _logp_gt = logp_gt[(_model.name, obj_name)]
    true_vline = hv.HoloMap(
        {d.name: hv.Curve([(gt[d.name], 0)], kdims=[d], label="true value")  # 1 point Curve won't plot, but will show up in the legend
                 * hv.VLine(gt[d.name], kdims=[d, 'p'])  # Annotations don't show up in legend
         for d in test.kdims},
                      kdims=['dimension']
        ).opts(hv.opts.Curve(color='#888888'),
               hv.opts.VLine(color='#888888'))
    frames[(_model.name, obj_name)] = \
        (test.mc_marginal_dists
         * test.p_marginals.opts(color='orange', line_width=3)
         * true_vline
        ) \
        .layout().cols(2).opts(axiswise=True) \
        .opts(hv.opts.Curve(axiswise=True),
              hv.opts.Distribution(axiswise=True),
              #hv.opts.VLine(color='#555555')
             )
    higher_than_gt = [('ground truth',
                       *(gt[d.name] for d in test.kdims),
                       _logp_gt,3)]
    higher_than_gt += \
       [(i, *s, l) 
        for i,s,l in zip(itertools.count(), test._samples, test._logps)
        if l >=  _logp_gt]
    higher_than_gt_frames[(_model.name, obj_name)] = \
        hv.Table(higher_than_gt, kdims=['index']+test.kdims, vdims=['log p'])

# %% [markdown] tags=["remove-cell"]
# Display results

# %% tags=["remove-cell"]
hvframes = hv.HoloMap(frames, kdims=[hv.Dimension('model'),
                                     hv.Dimension('objective', label="objective function")]) \
           .collate() \
           .opts(hv.opts.NdLayout(framewise=True, axiswise=True),
                 hv.opts.Overlay(framewise=True, axiswise=True))

# %% tags=["remove-input"]
hvframes

# %% [markdown] tags=["remove-cell"]
# The following code can be used to export plots and allow them to be imported into another notebook.
# ```python
# from holoviews.core.io import Pickler
#
# model_str = "-".join(model for model in models)
# obj_fn_str = "-".join(obj for obj in obj_fns)
# filename = f"one_step_forward__{model_str}__{obj_fn_str}"
# #hv.save(frames, f"{filename}.html")  # Save a file containing just the plot
#
# Pickler.save(frames, f"{filename}.hvz")  # Save the plot so it can be imported in another notebook
# ```

# %% [markdown]
# Check if there are any samples with higher likelihood than the ground truth values. While small deviations of the mode due to the prior may be expected, any moderate deviation may be indicative of a problem.

# %% tags=["remove-input"]
# Round the values for a more compact display
htgf = {}
for k, table in higher_than_gt_frames.items():
    df = table.data
    if not isinstance(df, pd.DataFrame):
        df = pd.DataFrame(df)
    else:
        df = df.copy()
    for c in df.columns:
        if c == 'index':
            pass
        elif c == 'log p':
            df[c] = round(df[c], 3)
        else:
            df[c] = round(df[c], 4)
    htgf[k] = hv.Table(df)

table_frames = hv.HoloMap(htgf,
                          kdims=[hv.Dimension('model'),
                                 hv.Dimension('objective', label="objective function")])
table_frames.opts(title="Sample points with likelihood higher than the ground truth")
table_frames

# %% tags=["remove-input"]
GitSHA()
