# -*- coding: utf-8 -*-
# ---
# jupyter:
#   jupytext:
#     formats: py:percent
#     notebook_metadata_filter: -jupytext.text_representation.jupytext_version
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
#   kernelspec:
#     display_name: Python (sinnfull)
#     language: python
#     name: sinnfull
# ---

# %% [markdown]
# # One-step-forward likelihood test
#
# This notebook performs a simple but effective consistency check between a model's update equations and the likelihood used to fit that model. For the purpose of this test, variable dimensions are concatenated, such that if the model involves 2 variables with respectively 2 and 3 dimensions, we treat it as predicting an $M=5$ dimensional distribution. For some given $t$, the goal is to estimates the $M$ marginals of $p(X_{t+1}^m | X_t^m)$ in two ways:
#
# 1. By integrating the model up to $t$, and then running the simulator for time $t+1$ $N$ times. Binning the results (using NumPy's 'auto' algorithm to determine the bin edges $\{\bar{x}^m\}$) gives estimates
#   $$\hat{p}_{\text{sim}}(\bar{x}^m_l \leq X_{t+1}^m < \bar{x}^m_{l+1} | X_t=\{x_t\}) \,.$$
# 2. By evaluating the objective function at each of the points generated by the simulator, giving $p_{\text{obj}}(X_{t+1}=x_{t+1} | X_t=\{x_t\})$. Taking the same bin edges, we average within each bin to get
#   $$\hat{p}_{\text{obj}}(\bar{x}^m_l \leq X_{t+1}^m < \bar{x}^m_{l+1} | X_t=\{x_t\}) = \frac{1}{|B^m_l|} \sum_{x_i \in B^m_l} p_{\text{obj}}(X_{t+1}=x_{t+1} | X_t=\{x_t\})\,,$$
#   where $B^m_l = \{x_i | \bar{x}^m_i \leq x^m_i < \bar{x}^m_{i+1}\}$ is the $l$-th bin for dimension $m$.
#
# For (1), we can in fact do better, and use a kernel density estimator instead of a histogram. This produces a smooth distribution, which is easier to visually compare to the histograms of (2) when the two distributions are plotted over each other. If we see any mismatch between the distributions, fits are unlikely to produce meaningful results.
#
# The manner in which we calculate the distributions (1) and (2) is fully described in [the implementation notebook](../diagnostics/one_step_cost.ipynb).

# %%
import sinnfull
sinnfull.setup('numpy', view_only=True) # This test is around 2.5 faster with numpy vs theano

# %%
from sinnfull.diagnostics.one_step_cost import OneStepForwardLogp

# %%
import holoviews as hv
hv.extension('bokeh')

# %% [markdown]
# Having calculated the estimates of $P(X_{t+1} | X_t)$ based on the simulation (1) and the likelihood (objective function) (2), to see if they match. Note that if we are using a true likelihood, these really should match exactly; errors on secondary terms in either the simulation or likelihood equations may cause only slight variations.
#
# Beyond implementation correctness, the shape of this distribution can also help identify issues when fitting latents. Distributions which are strongly peaked along one component will tend to favour fitting that component to the detriment of others. Moreover, if the support of $P(X_{t+1} | X_t)$ is too small along a given component (which may be related to it being too peaked), then when simulations are far from observations, there may be little signal with which to compute gradients.
#
# Limitations:
# - These distributions are drawn for one particular time point, for one particular set of parameters. It is worth running this test multiple times with different parameters. Whether it is also worth running the test for multiple time points depends on whether the system is ergodic.
# - “Peakiness” is only indicative for the latent fit. For a similar assessment of likelihood over parameters, one would have to plot distributions over the parameters.

# %%
#models = ['OUInput', 'OUInput2', 'OUInput3', 'OUInput4']
models = ['OUInput5']
obj_fns = ['logp_forward_se']
num_test_samples = 1000

# %%
frames = {}
for model_name in models:
    for obj_fn in obj_fns:
        test = OneStepForwardLogp(
            model_name = 'OUInput', logp_name = 'logp_forward_se', T = 1., step = 0.01)
        test.set_t(0.5)
        test.sample_next_step(num_test_samples)
    frames[(model_name, obj_fn)] = \
        (test.sim_marginal_dists * test.p_marginals.opts(color='orange', line_width=3)) \
          .layout().cols(2).opts(axiswise=True) \
          .opts(hv.opts.Curve(axiswise=True), hv.opts.Area(axiswise=True))

# %%
frames = hv.HoloMap(frames, kdims=[hv.Dimension('model'),
                                   hv.Dimension('objective', label="objective function")]) \
         .collate() \
         .opts(framewise=True)

# %%
frames

# %% [markdown]
# ```python
# from holoviews.core.io import Pickler
#
# model_str = "-".join(model for model in models)
# obj_fn_str = "-".join(obj for obj in obj_fns)
# filename = f"one_step_forward__{model_str}__{obj_fn_str}"
# #hv.save(frames, f"{filename}.html")  # Save a file containing just the plot
#
# Pickler.save(frames, f"{filename}.hvz")  # Save the plot so it can be imported in another notebook
# ```

# %%
